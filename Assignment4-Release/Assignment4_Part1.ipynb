{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ik9xIzUq9AmO"
      },
      "source": [
        "#1.  GAN generation and manipulation "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup GenForce\n",
        "GenForce is an open source library for generative model. For this assignment, we will mainly focus on StyleGAN2. "
      ],
      "metadata": {
        "id": "JinmfiDfc476"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UEkizSfB74uh"
      },
      "outputs": [],
      "source": [
        "# Do not change this code block\n",
        "import os\n",
        "os.chdir('/content')\n",
        "CODE_DIR = 'GenForce'\n",
        "!git clone https://github.com/genforce/genforce.git $CODE_DIR\n",
        "os.chdir(f'./{CODE_DIR}')\n",
        "!pip install -r requirements.txt > installation_output.txt\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "import io\n",
        "import IPython.display\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "import torch\n",
        "\n",
        "from models import MODEL_ZOO\n",
        "from models import build_generator\n",
        "from utils.visualizer import fuse_images\n",
        "import tqdm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SE-KRU5rAlh3"
      },
      "source": [
        "# Load StyleGAN2 Model for human face\n",
        "First we need to initialize the StyleGAN2 model and load the checkpoint pre-trained on FF-HQ. We will use the model with 256*256 resolution to save time. If you are seeking better generation quality, you may change to 512 or 1024 resolution after you finish all the implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZqvNJ4wAjEs"
      },
      "outputs": [],
      "source": [
        "# Do not change this code block\n",
        "# Download checkpoint, this should take ~30s\n",
        "model_name = \"stylegan_ffhq256\"\n",
        "model_url = \"https://mycuhk-my.sharepoint.com/:u:/g/personal/1155082926_link_cuhk_edu_hk/ES-NAUCC2qdHg87BftvlBiQBVpbJ8-005Q4TNr5KrOxQEw?e=00AnWt&download=1\"\n",
        "os.makedirs('checkpoints', exist_ok=True)\n",
        "checkpoint_path = os.path.join('checkpoints', model_name + '.pth')\n",
        "subprocess.call(['wget', '-O', checkpoint_path, model_url])\n",
        "\n",
        "# Initialize StyleGAN generator\n",
        "model_config = MODEL_ZOO[model_name].copy()\n",
        "model_config.pop('url')\n",
        "generator = build_generator(**model_config)\n",
        "generator = generator.cuda()\n",
        "generator.eval()\n",
        "\n",
        "# Load checkpoint\n",
        "checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
        "generator.load_state_dict(checkpoint['generator'])\n",
        "print(f'Finish loading checkpoint.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLwZaJzb88sI"
      },
      "source": [
        "# Define utility functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5T0ljCVz9i7Q"
      },
      "outputs": [],
      "source": [
        "# Do not change this code block\n",
        "def postprocess(images):\n",
        "  \"\"\"Post-processes images from `torch.Tensor` to `numpy.ndarray`.\"\"\"\n",
        "  images = images.detach().cpu().numpy()\n",
        "  images = (images + 1) * 255 / 2\n",
        "  images = np.clip(images + 0.5, 0, 255).astype(np.uint8)\n",
        "  images = images.transpose(0, 2, 3, 1)\n",
        "  return images\n",
        "\n",
        "def imshow(images, viz_size=256, col=0, spacing=0):\n",
        "  \"\"\"Shows images in one figure.\"\"\"\n",
        "  fused_image = fuse_images(\n",
        "    images,\n",
        "    col=col,\n",
        "    image_size=viz_size,\n",
        "    row_spacing=spacing,\n",
        "    col_spacing=spacing\n",
        "  )\n",
        "  fused_image = np.asarray(fused_image, dtype=np.uint8)\n",
        "  data = io.BytesIO()\n",
        "  PIL.Image.fromarray(fused_image).save(data, 'jpeg')\n",
        "  im_data = data.getvalue()\n",
        "  disp = IPython.display.display(IPython.display.Image(im_data))\n",
        "  return disp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nq7Qgu6CotRN"
      },
      "source": [
        "# Generate 10 Image and store the corresponding latent codes (both z latent and w latent)\n",
        "This is a sample code for you for generating 10 human face images. StyleGAN2 has two latent space, z and w, and we will test both latent space on their disentangling ability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KCb3chNHotXK"
      },
      "outputs": [],
      "source": [
        "# Do not change this code block\n",
        "# Set random seed.\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "num = 10\n",
        "batch_size = 1\n",
        "\n",
        "# data structure to store latent codes (both z and w), you may use any structure as you like\n",
        "z = []\n",
        "w = []\n",
        "\n",
        "# Sample and synthesize.\n",
        "outputs = []\n",
        "for idx in tqdm.tqdm(range(0, num, batch_size)):\n",
        "  batch = min(batch_size, num - idx)\n",
        "  latent_z = torch.randn(batch, generator.z_space_dim).cuda()\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    latent_w = generator.mapping(latent_z)['w']\n",
        "    z.append(latent_z.detach().cpu())\n",
        "    w.append(latent_w.detach().cpu())\n",
        "    wp = generator.truncation(latent_w)\n",
        "    images = generator.synthesis(wp)['image']\n",
        "    images = postprocess(images)\n",
        "  outputs.append(images)\n",
        "img_tensor = np.concatenate(outputs, axis=0)\n",
        "\n",
        "\n",
        "imshow(img_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSVhS0Dr8-L_"
      },
      "source": [
        "# Extract face attributes\n",
        "We will use deepface API to extract face attributes for each image.\n",
        "The code below is a simple example of using deepface API to extract age and emotion attribute of the 10 person generated in previous section. Your task is:\n",
        "\n",
        "1. Generate 10k images using StyleGAN, store the z and w latent code\n",
        "2. Get the age and emotion attributes of the 10k generated images, and store the attributes.\n",
        "3. Process the attributes to be binary label. For age attribute, label age > 20 as 1 and age < 20 as -1. For emotion attribute, label \"happy\" as 1 and the other as -1.\n",
        "\n",
        "You can choose any data structure as you like. We suggest to store the label and latent codes in the drive since getting them might take a long time and you need to re-run when resume your work, but it is not required."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4EOcz-RluPMf"
      },
      "outputs": [],
      "source": [
        "!pip install deepface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-9a7JISupnZ"
      },
      "outputs": [],
      "source": [
        "from deepface import DeepFace\n",
        "attribute = DeepFace.analyze(img_path = [img_tensor[1], img_tensor[2]], actions = ['age','emotion'], enforce_detection = False, prog_bar = False)\n",
        "attribute"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQAPk_XI3tON"
      },
      "source": [
        "## 1. Generate 5k latent code (z and w)   \n",
        "Similar to the provided example, please generate 5k images and store the z and w latent code seperately. Visualization might help you to debug but it is not required."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2Mp3xF54TnO"
      },
      "outputs": [],
      "source": [
        "def get_latent(generator):\n",
        "  #################################\n",
        "  ##### your code starts here #####\n",
        "  #################################\n",
        "\n",
        "  #################################\n",
        "  ##### your code ends here #######\n",
        "  #################################\n",
        "  return latent_z_list, latent_w_list, img_tensor\n",
        "latent_z_list, latent_w_list, img_tensor = get_latent(generator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyM6wTux4WWB"
      },
      "source": [
        "## 2. Get attributes    \n",
        "Get labels for the generated images. This part might take 30 minutes to complete. For more info about deepface attributes detection, you can check https://github.com/serengil/deepface and https://github.com/serengil/deepface/blob/master/deepface/DeepFace.py#L267\n",
        "\n",
        "Store the attributes and return."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpTmSfcT4bIB"
      },
      "outputs": [],
      "source": [
        "# we need to convert image tensor to image list for deepface \n",
        "def tensor2list(img_tensor):\n",
        "  img_list = []\n",
        "  for tensor in img_tensor:\n",
        "    img_list.append(tensor)\n",
        "  return img_list\n",
        "\n",
        "def get_attribute(img_tensor):\n",
        "  img_list = tensor2list(img_tensor)\n",
        "  #################################\n",
        "  ##### your code starts here #####\n",
        "  #################################\n",
        "\n",
        "  #################################\n",
        "  ##### your code ends here #######\n",
        "  #################################\n",
        "  \n",
        "  return attribute\n",
        "\n",
        "attribute = get_attribute(img_tensor)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwO2KckJ4dYg"
      },
      "source": [
        "## 3. Process attributes to get binary labels \n",
        "Process the attributes to be binary label. For age attribute, label age > 20 as 1 and age < 20 as -1. For emotion attribute, label \"happy\" as 1 and the other as -1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxqEexaM4mxF"
      },
      "outputs": [],
      "source": [
        "def get_label(raw_attributes):\n",
        "  #################################\n",
        "  ##### your code starts here #####\n",
        "  #################################\n",
        "\n",
        "  #################################\n",
        "  ##### your code ends here #######\n",
        "  #################################\n",
        "\n",
        "  return label_age, label_emotion\n",
        "\n",
        "label_age, label_emotion = get_label(attribute)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iB7Ya6tRV7NO"
      },
      "source": [
        "# InterfaceGAN\n",
        "[InterfaceGAN ](https://arxiv.org/abs/2005.09635) is a simple approach to manipulate the latent space. It takes latent codes as training data, their corresponding attribute as labels, and train a SVM to classify the latent codes from the labels, then extract the classification boundary as the attribute boundary for StyleGAN. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G85lfQMv4xmy"
      },
      "source": [
        "## Get decision boundary \n",
        "In this part we will implemente InterfaceGAN to derive the boundary of each attributes. Different from original paper which uses SVM, we will instead use logistic regression since SVM is not covered in class. The latent code should be your data, and apply logistic regression with the binary label. You may re-use any code from previous assignments, or you can use functions in sklearn. You should get FOUR boundaries: z_age_boundary, z_emotion_boundary, w_age_boundary, w_emotion_boundary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8OfPu_51lzK"
      },
      "outputs": [],
      "source": [
        "import sklearn.linear_model\n",
        "def get_boundary(latent, label):\n",
        "  #################################\n",
        "  ##### your code starts here #####\n",
        "  #################################\n",
        "\n",
        "  #################################\n",
        "  ##### your code ends here #######\n",
        "  #################################\n",
        "z_age_boundary = get_boundary(latent_z_list, label_age)\n",
        "z_emotion_boundary = get_boundary(latent_z_list, label_emotion)\n",
        "w_age_boundary = get_boundary(latent_w_list, label_age)\n",
        "w_emotion_boundary = get_boundary(latent_w_list, label_emotion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPx82W2Q41Rd"
      },
      "source": [
        "## Apply InterfaceGAN \n",
        "To apply InterfaceGAN, add the initial latent code with the boundary such that w_elder_latent = w_latent + scale * w_age_boundary where you can choose your own scale parameter to make the generated image better. We are able to get a decent result by setting scale=2. Feed the \"editted\" latent to the generator and visualize the result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HT7zJj3l9FmQ"
      },
      "outputs": [],
      "source": [
        "def apply_interfacegan(generator, age_boundary, emotion_boundary, scale):\n",
        "  #################################\n",
        "  ##### your code starts here #####\n",
        "  #################################\n",
        "\n",
        "  #################################\n",
        "  ##### your code ends here #######\n",
        "  #################################\n",
        "  return img_tensor, img_tensor_edit_age, img_tensor_edit_emotion\n",
        "  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9o70i2rJIGK"
      },
      "source": [
        "## Visualize your result\n",
        "The face might not be high quality since we are using the StyleGAN model of resolution 256 for fast inference time. If you are interested in higher quality for image generation or InterfaceGAN, feel free to change the resolution to 1024.\n",
        "\n",
        "Here we provide an expected output. As long as your manipulation makes sense you will get full credit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RgLavdpMJKn9"
      },
      "outputs": [],
      "source": [
        "raw, w_age_editted, w_emotion_editted = apply_interfacegan(generator, w_age_boundary, w_emotion_boundary, scale)\n",
        "print(\"Raw image\")\n",
        "imshow(raw)\n",
        "print(\"Younger face\")\n",
        "imshow(w_age_editted)\n",
        "print(\"Happier face\")\n",
        "imshow(w_emotion_editted)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Interpolation \n",
        "Generate the Interpolation of age and happy attribute. You should first generate 1 image, edit it using age and happy boundary with scale from [-5, 5]. Return the interpolated image tensor. "
      ],
      "metadata": {
        "id": "qdrP25wMeRiO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def interpolation(generator, boundary):\n",
        "  scale = np.arange(-5, 5, 1)\n",
        "  interpolation = []\n",
        "  #################################\n",
        "  ##### your code starts here #####\n",
        "  #################################\n",
        "  \n",
        "  #################################\n",
        "  ##### your code ends here #######\n",
        "  #################################\n",
        "  img_tensor = np.concatenate(interpolation, axis=0)\n",
        "  return img_tensor"
      ],
      "metadata": {
        "id": "QFrbEv7yeQ7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imshow(img_tensor)"
      ],
      "metadata": {
        "id": "9VImG7bnfdst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question: From your observation, which latent space is more powerful (disentangled) for image manipulation? w space or z space?"
      ],
      "metadata": {
        "id": "dZVNLOPSay3I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Your Anwser: "
      ],
      "metadata": {
        "id": "fm618U7UbCZp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GAN Inversion\n",
        "Now we went through how to generate image from a random latent code, and manipulate the latent code to edit images. However, many real-life application requires to have an image as input and edit the image, that is, \"encode\" the image into GAN's latent space. We call this process GAN Inversion. The idea of GAN Inversion is to first generate the image from a random latent code, and calculate the loss between the generated image and the real image, then backprogate through a neural network (VGG) to optimize the latent code until the output images are similar enough."
      ],
      "metadata": {
        "id": "qlJn6SO4BRwW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. process input image\n",
        "Upload img.jpg to your colab, and copy the path to read image"
      ],
      "metadata": {
        "id": "k2hgMfvCkclA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9t0yPR1xGoZF"
      },
      "outputs": [],
      "source": [
        "# process input image\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "# Opens a image in RGB mode\n",
        "input_image = Image.open(r\"/content/img.jpg\")#.resize((256, 256))\n",
        "input_image_vis = np.asarray(input_image.resize((256, 256))).reshape(1,256,256,3)\n",
        "\n",
        "# transform\n",
        "transform = transforms.Compose(\n",
        "        [\n",
        "            transforms.Resize((256, 256)),\n",
        "            transforms.CenterCrop((256, 256)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "input_image = transform(input_image)\n",
        "imshow(input_image_vis)\n",
        "\n",
        "def vis(input_img):\n",
        "  wp = generator.truncation(input_img.cuda())\n",
        "  images = generator.synthesis(wp)['image']\n",
        "  images = postprocess(images)\n",
        "  imshow(images)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. build VGG network"
      ],
      "metadata": {
        "id": "xE_QaIankjYq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.models as models\n",
        "vgg16 = models.vgg16(pretrained=True)"
      ],
      "metadata": {
        "id": "PPzl2pUvkmrz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Get mean latent and visualize the mean face\n",
        "Optimizing from pure random variable is very difficult. Instead, we first get the mean latent of 10k generated face to represent an average face of human and start the optimization from it."
      ],
      "metadata": {
        "id": "-FOIoS2elELs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "num = 10000\n",
        "batch_size = 1\n",
        "\n",
        "w_samples = []\n",
        "\n",
        "# Sample and synthesize.\n",
        "outputs = []\n",
        "for idx in tqdm.tqdm(range(0, num, batch_size)):\n",
        "  batch = min(batch_size, num - idx)\n",
        "  latent_z = torch.randn(batch, generator.z_space_dim).cuda()\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    latent_w = generator.mapping(latent_z)['w']\n",
        "    w_samples.append(latent_w.detach().cpu())\n",
        "    \n",
        "w_avg = np.mean(w_samples, axis=0, keepdims=True)\n",
        "w_std = (np.sum((w_samples - w_avg) ** 2) / num) ** 0.5\n",
        "\n",
        "vis(w_avg[0].cuda())"
      ],
      "metadata": {
        "id": "omTWCTQ4j1sn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Use VGG to Get image feature"
      ],
      "metadata": {
        "id": "itfFljf3OQC8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_image = input_image.cuda()\n",
        "vgg16.eval().cuda()\n",
        "\n",
        "#################################\n",
        "##### your code starts here #####\n",
        "#################################\n",
        "target_features = \n",
        "#################################\n",
        "##### your code ends here #######\n",
        "#################################\n",
        "\n"
      ],
      "metadata": {
        "id": "NaEqggW-yPsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Noise and optimizer initialization"
      ],
      "metadata": {
        "id": "Uz0ryUWAyYL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "noise_bufs = { name: buf for (name, buf) in generator.synthesis.named_buffers() if 'apply_noise' in name }\n",
        "\n",
        "w_opt = torch.tensor(w_avg[0], dtype=torch.float32, device=device, requires_grad=True) \n",
        "optimizer = torch.optim.Adam([w_opt] + list(noise_bufs.values()), betas=(0.9, 0.999), lr=initial_learning_rate)\n",
        "\n",
        "# Init noise.\n",
        "for buf in noise_bufs.values():\n",
        "    buf.requires_grad = False\n",
        "\n",
        "    buf[:] = torch.randn_like(buf)\n",
        "    buf.requires_grad = True"
      ],
      "metadata": {
        "id": "cysMXGyoytES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Hyperparameter setup"
      ],
      "metadata": {
        "id": "fCiOISLBytzy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "device = \"cuda\"\n",
        "num_steps                  = 1500\n",
        "initial_learning_rate      = 0.05\n",
        "initial_noise_factor       = 0.05\n",
        "lr_rampdown_length         = 0.25\n",
        "lr_rampup_length           = 0.05\n",
        "noise_ramp_length          = 0.75\n",
        "regularize_noise_weight    = 1e5\n",
        "\n",
        "input_image = input_image.cuda()\n",
        "vgg16.eval().cuda()\n"
      ],
      "metadata": {
        "id": "7MFN7-Khy2AV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Start optimization process\n",
        "We start get the inversion by optimization. The inverted image by this method might not look like our target, (and usually look completely different). Don't worry too much about the final result, as long as the implementation and the result makes sense you will get full credit.\n",
        "\n",
        "Your task in this part are:\n",
        "\n",
        "*    Get synth_images from w latent code\n",
        "*    Get synthesis feature from synth_images\n",
        "*    Implement MSE loss between target feature and synthesis feature for the optimizaiton \n",
        "*    Tune the hyperparameter to get a \"good enough\" result. It's fine that your result looks very different from the reference image. However, you are expected to get a human image with some ideneities similar to the reference image (e.g. race). "
      ],
      "metadata": {
        "id": "CYIGApiZy6C6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for step in tqdm.tqdm(range(num_steps)):\n",
        "    # Learning rate schedule.\n",
        "    t = step / num_steps\n",
        "    w_noise_scale = w_std * initial_noise_factor * max(0.0, 1.0 - t / noise_ramp_length) ** 2\n",
        "    lr_ramp = min(1.0, (1.0 - t) / lr_rampdown_length)\n",
        "    lr_ramp = 0.5 - 0.5 * np.cos(lr_ramp * np.pi)\n",
        "    lr_ramp = lr_ramp * min(1.0, t / lr_rampup_length)\n",
        "    lr = initial_learning_rate * lr_ramp\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    # Synth images from opt_w.\n",
        "\n",
        "    w_noise = torch.randn_like(w_opt) * w_noise_scale.cuda()\n",
        "    w_latent = (w_opt + w_noise).repeat([1, 1, 1])\n",
        "\n",
        "    #################################\n",
        "    ##### your code starts here #####\n",
        "    #################################\n",
        "    synth_images = \n",
        "    # Features for synth images.\n",
        "    synth_features = \n",
        "    # MSE Loss\n",
        "    mse_loss = \n",
        "    #################################\n",
        "    ##### your code ends here #######\n",
        "    #################################\n",
        "\n",
        "    \n",
        "    \n",
        "    # Noise regularization.\n",
        "    reg_loss = 0.0\n",
        "    for v in noise_bufs.values():\n",
        "        noise = v[None,None,:,:] # must be [1,1,H,W] for F.avg_pool2d()\n",
        "        while True:\n",
        "            reg_loss += (noise*torch.roll(noise, shifts=1, dims=3)).mean()**2\n",
        "            reg_loss += (noise*torch.roll(noise, shifts=1, dims=2)).mean()**2\n",
        "            if noise.shape[2] <= 8:\n",
        "                break\n",
        "            noise = F.avg_pool2d(noise, kernel_size=2)\n",
        "    loss = mse_loss + reg_loss * regularize_noise_weight\n",
        "\n",
        "    # Step\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward(retain_graph=True)\n",
        "    optimizer.step()\n",
        "   \n",
        "\n",
        "    # Save projected W for each optimization step.\n",
        "    w_out = w_opt.detach()[0]\n",
        "\n",
        "    # Normalize noise.\n",
        "    with torch.no_grad():\n",
        "        for buf in noise_bufs.values():\n",
        "            buf = buf - buf.mean()\n",
        "            buf = buf * buf.square().mean().rsqrt()\n"
      ],
      "metadata": {
        "id": "DGrZkYpbl8rt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vis(w_out.unsqueeze(0))"
      ],
      "metadata": {
        "id": "U_rxFoO7uEBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ssCCQ1vHxTvy"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Copy of Assignment4-Part1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}