{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KYT7oguFDn4H"
   },
   "source": [
    "# Assignment 4 - Part2\n",
    "\n",
    "In this assignment you will train a semantic segmentation module, and then come up with your own segmentation model.\n",
    "\n",
    "You can refer to https://github.com/CSAILVision/semantic-segmentation-pytorch for more codes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DZyWdzaaWILA"
   },
   "source": [
    "# Setup Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rncQ8P3AM5ZV"
   },
   "source": [
    "First, download the miniplaces images folder (which we did for last assignment). Zip it, upload it to the assignment folder, and unzip it below.\n",
    "(The unzipping process will take about 5 minutes.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YjzsBhslWJFZ"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "twuuuKg3Yc0P"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hLir_mUQYi6d"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# TODO: Fill in the Google Drive path where you uploaded the assignment\n",
    "# Example: If you create a 188 folder and put all the files under Assignment1 folder, then '188/Assignment1'\n",
    "# GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = '188/Assignment1'\n",
    "GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = '188/Assignment4'\n",
    "GOOGLE_DRIVE_PATH = os.path.join('drive', 'My Drive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n",
    "print(os.listdir(GOOGLE_DRIVE_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RIzoPO3zY1Mf"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(GOOGLE_DRIVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tkOmNbPdSqLm"
   },
   "source": [
    "Now we are going to untar the actions folder. Don't worry! This time the file is much smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D9svhBylSd28"
   },
   "outputs": [],
   "source": [
    "!tar -xvf \"/content/drive/My Drive/188/Assignment4/annotations.tar.xz\" -C \"/content/drive/My Drive/188/Assignment4/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z_mnBNRiIgcy"
   },
   "outputs": [],
   "source": [
    "!tar -xvf \"/content/drive/My Drive/188/Assignment4/images.tar.xz\" -C \"/content/drive/My Drive/188/Assignment4/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iQd3K0MyDn4I"
   },
   "source": [
    "### Train ResNet + UPerNet\n",
    "First fill the codes in model.py and train the semantic segmentation module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UgNBHLThDn4J"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "\n",
    "from model import *\n",
    "from dataset import *\n",
    "from train import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8-K3fC2NDn4J"
   },
   "outputs": [],
   "source": [
    "net_encoder = Resnet().cuda()\n",
    "net_decoder = UPerNet().cuda()\n",
    "\n",
    "crit = nn.NLLLoss(ignore_index=-1)\n",
    "\n",
    "dataset_train = ADEDataset(GOOGLE_DRIVE_PATH, 'training')\n",
    "dataset_val = ADEDataset(GOOGLE_DRIVE_PATH, 'validation')\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset_train,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    num_workers=2)\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset_val,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    num_workers=2)\n",
    "\n",
    "optimizer_encoder = torch.optim.SGD(\n",
    "    group_weight(net_encoder),\n",
    "    lr=0.02,\n",
    "    momentum=0.9,\n",
    "    weight_decay=1e-4)\n",
    "\n",
    "optimizer_decoder = torch.optim.SGD(\n",
    "    group_weight(net_decoder),\n",
    "    lr=0.02,\n",
    "    momentum=0.9,\n",
    "    weight_decay=1e-4)\n",
    "\n",
    "for epoch in range(10): #choose smaller number if you are out of GPU\n",
    "    loss, acc = train_seg(train_dataloader, crit, net_encoder, net_decoder, optimizer_encoder, optimizer_decoder)\n",
    "\n",
    "    print (\"Epoch %d, trainnig acc %f, training loss %f\"%(epoch, acc, loss))\n",
    "\n",
    "    val_loss, val_acc = val_seg(val_dataloader, crit, net_encoder, net_decoder)\n",
    "    print (\"Epoch %d, validation acc %f, validation loss %f\"%(epoch, val_acc, val_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AC6trlvY-n2W"
   },
   "source": [
    "I can get 64% val acc. what about you?\n",
    "\n",
    "### Visualizaton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bEU1vmA-Dn4L"
   },
   "outputs": [],
   "source": [
    "# System libs\n",
    "import os, csv, torch, numpy, scipy.io, PIL.Image, torchvision.transforms\n",
    "\n",
    "def colorEncode(labelmap, colors, mode='RGB'):\n",
    "    labelmap = labelmap.astype('int')\n",
    "    labelmap_rgb = np.zeros((labelmap.shape[0], labelmap.shape[1], 3),\n",
    "                            dtype=np.uint8)\n",
    "    for label in numpy.unique(labelmap):\n",
    "        if label < 0:\n",
    "            continue\n",
    "        labelmap_rgb += (labelmap == label)[:, :, np.newaxis] * \\\n",
    "            np.tile(colors[label],\n",
    "                    (labelmap.shape[0], labelmap.shape[1], 1))\n",
    "\n",
    "    if mode == 'BGR':\n",
    "        return labelmap_rgb[:, :, ::-1]\n",
    "    else:\n",
    "        return labelmap_rgb\n",
    "    \n",
    "colors = scipy.io.loadmat('color150.mat')['colors']\n",
    "names = {}\n",
    "with open('objectInfo150.txt') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "    for line in lines[1:]:\n",
    "        row = line.strip().split()\n",
    "        names[int(row[0])] = row[4]\n",
    "\n",
    "def visualize_result(img, pred, index=None):\n",
    "    # filter prediction class if requested\n",
    "    if index is not None:\n",
    "        pred = pred.copy()\n",
    "        pred[pred != index] = -1\n",
    "        print(f'{names[index+1]}:')\n",
    "        \n",
    "    # colorize prediction\n",
    "    pred_color = colorEncode(pred, colors).astype(numpy.uint8)\n",
    "\n",
    "    # aggregate images and save\n",
    "    im_vis = numpy.concatenate((img, pred_color), axis=1)\n",
    "    display(PIL.Image.fromarray(im_vis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xtgjOiPeDn4M"
   },
   "outputs": [],
   "source": [
    "pil_to_tensor = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406], # These are RGB mean+std values\n",
    "        std=[0.229, 0.224, 0.225])  # across a large photo dataset.\n",
    "])\n",
    "pil_image = PIL.Image.open('ADE_val_00001519.jpg').convert('RGB')\n",
    "img_original = numpy.array(pil_image)\n",
    "img_original = imresize(img_original, (128, 128))\n",
    "img_original = PIL.Image.fromarray(img_original)\n",
    "img_data = pil_to_tensor(img_original)\n",
    "\n",
    "singleton_batch = img_data[None].cuda()\n",
    "output_size = img_data.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zIdZTBiWDn4N"
   },
   "outputs": [],
   "source": [
    "net_decoder.use_softmax = True\n",
    "pred = net_decoder(net_encoder(singleton_batch), segSize=output_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7KS4ThAEDn4N"
   },
   "outputs": [],
   "source": [
    "_, pred = torch.max(pred, dim=1)\n",
    "pred = pred.cpu()[0].numpy()\n",
    "\n",
    "visualize_result(img_original, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WqV3wUq9-_xQ"
   },
   "source": [
    "## Build your own model\n",
    "\n",
    "you can replace the encoder and decoder with any model you find in https://github.com/CSAILVision/semantic-segmentation-pytorch.\n",
    "\n",
    "Train the model.\n",
    "\n",
    "Compare the results."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
